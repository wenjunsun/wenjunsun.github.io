# PCA

## What is PCA and what is it used for?

PCA stands for principal component analysis. It is probably the simplest way to do dimensionality reduction. Suppose we have a dataset with 100 features. Fitting a machine learning model on those 100 features might not yield the best model, because some features give the same information. (For example, height and weight are positively correlated) So how do we know which of the 10 features out of the 100 features are the most important ones? what do we mean by “important”? We meant how important this feature is in helping the model predict the response variable, whatever that is. In PCA, the metric is variation. If after reducing from 100 features to 10 features the data are very spread out, we consider to be a good reduction. How to capture this “spread” mathematically? We use variance. Let’s look at an example to make this idea clearer.

<img src="/images/image1.png" style="width:2.98859in;height:2.9375in" />

Say in the above picture of 2-dimensional data, we would like to reduce data to 1-dimension. Which 1-dimension do we choose? Should we choose the green axis to reduce data onto, or the purple one or some other axis? It is intuitive that we should project data onto the green axis, right? Because after projecting the data to the green axis, the projected data retains the most variation. (more variation in data means more interesting). Another way to see it is that the sum of distances from all the original points before projecting and after projecting is the smallest. Mathematically, this reconstruction error is minimal after projection when the variance within the projected space is the largest.

So now we know that PCA is used for reducing the dimension of the feature space, and it does so by finding the right linear subspace of the original linear space to project to, so to maximize variance of data within the projected space. How do we find the right subspace to project to? Well, if the data looks like the one above, we can just draw the green axis by looking at it right? Yes, we can do that. But this intuitive eyeballing goes out of the window when we try to reduce dataset from 100 dimensions to 10 dimensions. (How do you even draw a 10-dimensional space? I think even Einstein can only see 4 dimensions lol.) So, to find the right subspace to project our data to so to maintain the most information, we turn to math and linear algebra.

Note that PCA is not supervised learning algorithm. In our description above, we only looked at the input space and never looked at the labels. PCA is useful when we want to preprocess the data in a way such that dataset retains the most information possible with less features.

##  Deriving that PCA optimization is equivalent to eigenvalue decomposition

So, let us find a mathematical formula that will allow us to find the subspace that we were talking about above! First, let us think back to the definition of variance. Mathematically, Var(*X*) = *E*\[(*X*−*E*\[*X*\])<sup>2</sup>\], which just means the average squared distance of all data points to the center of the data. Here we make a simplifying assumption that the mean of our dataset is 0. Even if the mean of our data is not 0, we can make it to be 0 by shifting the frame of reference. So now the contribution to variance by a datapoint will just be its squared distance to origin. So, we want the kind of projection such that after that $\\frac{1}{m}\\sum\_{i}^{}\\left\\\| x^{\\left( i \\right)} \\right\\\|^{2}$is maximized. (where m is number of points we have) ∥ ⋅ ∥ is just a norm operator, telling us how long this vector is. The longer this vector is, the better (because we are maximizing variation). Now how do we represent the projection? Anyone who has taken linear algebra class knows that projection is a linear operation. So there must be a matrix that can represent it. The formula for projecting into a subspace that is spanned by vectors *x*<sub>1</sub>, *x*<sub>2</sub>, … is *A*(*A*<sup>*T*</sup>*A*)<sup> − 1</sup>*A*<sup>*T*</sup>, where *x*<sub>1</sub> is the first column of *A*, and *x*<sub>2</sub> is the second column of *A*, and so on.

So now our objective is to find a matrix *A*, such that $\\frac{1}{m}\\sum\_{i}^{}\\left\\\| A\\left( A^{T}A \\right)^{- 1}A^{T}x^{(i)} \\right\\\|^{2}$is maximized.

Now, from linear algebra we know there are infinite set of vectors that span a specific subspace. So let’s pick the vectors in *A* such that they are orthonormal, just to make our life easier. Now we have *A*<sup>*T*</sup>*A* = *I*.

So now we are picking *A* as to maximize

$$\sum_{i}\| AA^{T}x^{(i)} \|^{2}$$

We drop m because maximizing a number divided by a constant is the same as maximizing that number itself.

Now I don’t want to go too much into the mathematical details here, so I will just tell you the answer to the above optimization problem. Readers who are mathematically inclined can try to solve this by himself/herself. (Involves forming a generalized lagrangian with the constraint that A’s columns are orthonormal and taking a gradient and set that equal to 0.)

In the end we find that by the above metrics and goal, the condition for finding the best subspace of dimension *l* in a space of dimension *n* given a data matrix *X* (each data point is a row in the matrix), is that we want to find the *l* eigenvectors of *X*<sup>*T*</sup>*X* with the largest eigenvalues. And the subspace spanned by these *l* eigenvectors will be the subspace we desire.

So much about the mathematics side of it. Next time I will talk about the implementation of PCA in python.
